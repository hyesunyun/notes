# On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ

**Authors**: Emily M. Bender, Timnit Gebru, Angie McMillan-Major, and Shmargaret Shmitchell  
**Conference**: ACM FAccT Conference 2021 (Conference on Fairness, Accountability, and Transparency in Socio-Technical Systems)  
March 3-10, 2021  
**Links**: [PDF](https://dl.acm.org/doi/10.1145/3442188.3445922), [Slides](https://docs.google.com/presentation/d/16xC4OsvgVwuOnSn88eOXzzymLEHEFfZod21lLnsXHK0/edit?usp=sharing)  


## Introduction and Background

> One of the biggest trends in natural language processing (NLP) has been increasing the size of language models (LMs) as measured by the number of parameters and size of the training data.

1. BERT Large (2019) - 340 million parameters, 16 GB dataset size  
2. OpenAI's GPT-3 (2020) - 175 billion parameteres, 570 GB dataset size  
3. Google's GShard (2020) - 600 billion parameters  
4. Google's Switch-C (2021) - 1.6 trillion parameters, 745 GB dataset size  

**Questions this paper tries to address**:  
1. How big is too big?  
2. What are the possible risks associated with this technology (LMs) and what paths are available for mitigating those risks?  

> We hope that a critical overview of the risks of relying on ever-increasing size of LMs as the primary driver of increased performance of language technology can facilitate a reallocation of efforts towards approaches that avoid some of these risks while still reaping the benefits of improvements to language technology.

Lists already well-documented risks and harms as well as some that are new.

## Costs of Lanaguage Models

Need of efficiency as an evaluation metric, online tools to analyze energy usage, and pressure on companies to devise sustainable training techniques.

### Environmental Costs

While the average human is responsible for an estimated 5t CO<sub>2</sub>
emission per year, a Transformer (big) model with neural architecture search and estimated that the training procedure emitted 284t of CO<sub>2</sub> emission.

> Training a single BERT base model (without
hyperparameter tuning) on GPUs was estimated to require as much
energy as a trans-American flight.

> When we perform risk/benefit analyses of language technology,
we must keep in mind how the risks and benefits are distributed,
because they do not accrue to the same people.

> ... environmental racism that
the negative effects of climate change are reaching and impacting
the worldâ€™s most marginalized communities first.

### Financial Costs

**Cost of models vs their accuracy gains**

> For the task of machine translation where large
LMs have resulted in performance gains, they estimate that an
increase in 0.1 BLEU score using neural architecture search for
English to German translation results in an increase of $150,000
compute cost in addition to the carbon emissions.


## Large Training Data

Large, uncurated, Internet-based datasets encode the dominant/hegemnonic view, which further harms people at the margins. Many of these issues exist for LMs in general, but these risks and harms are greater for large models.

1. Internet access is not evenly distributed. 

> GPT-2â€™s training data is sourced by scraping outbound
links from Reddit, and Pew Internet Researchâ€™s 2016 survey
reveals 67% of Reddit users in the United States are men, and 64%
29.13 between ages 18 and 29.
> Wikipedians find that only 8.8-15% are women or girls.

2. Filtering datasets can further exclude voices of people at the margins. 

> If we filter out the discourse of marginalized populations, we fail to
provide training data that reclaims slurs and otherwise describes
marginalized identities in a positive light.

3. Social movements and discourse around them evolve over time. However, training data for many LMs are static.  

> Social movements produce new norms, language, and ways of communicating. This
adds challenges to the deployment of LMs, as methodologies reliant on LMs run the risk of â€˜value-lockâ€™, where the LM-reliant technology reifies older, less-inclusive understandings.

4. LMs encode bias and stereotypes against marginalize people.  

> Hutchinson et al. find that BERT associates phrases referencing persons with disabilities with more negative sentiment words, and that gun violence, homelessness, and drug addiction are overrepresented in texts discussing mental
illness.

5. Large trainig datsets often incur _documentation debt_, where datasets are both undocumented and too large to document post hoc.  

## Stochastic Parrots ðŸ¦œ

**False Coherence: Coherence in the Eye of the Beholder**. LMs are poor at communicating like humans who are able to take into context and intent into account when communicating with other humans. LMs don't truly understand language. We assign intent to a model that has none.
 
> Text generated by an LM is not grounded in communicative intent, any model of the world, or any model of the readerâ€™s state of mind. It canâ€™t have been, because the training data never included sharing thoughts with a listener, nor does the machine have the ability to do that.

> an LM is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic
information about how they combine, but without any reference to meaning: a stochastic parrot.

## Paths Forward

> We should consider our research time and effort a valuable resource, to be spent to the extent possible on research projects that build towards a technological ecosystem whose benefits are least evenly distributed or better accure to those historically most marginalized. This means considering how research contributions shape the overall direction of the field and keeping alert to directions that limit access.

**Proposed items**:  
1. Consider financial and environmental costs of model development up front  
2. Spend time assembling datasets suited for the tasks at hand  
3. Thorough documentation on the data used in model building  
4. Consider pre-mortems of how language models might fail  
5. Consider value sensitive design  
6. Explore multiple possible paths towards long-term goals. Large LMs might not be the only effective method.  
7. Investigate how large LMs can serve marginalized populations.  

## Criticism

**Links**: [Yoav Goldberg](https://gist.github.com/yoavg/9fc9be2f98b47c189a513573d902fb27), [Michael Lissack](https://arxiv.org/pdf/2101.10098.pdf)

1. The paper doesn't talk enough about work that's been done on improving the drawbacks listed.
2. For bias and stochastic parroting, the size doesn't matter so much as dataset quality.
3. The paper has political/ethical assumptions and is not up front about them. The paper presents an argument, not research.
